{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "188MxOVzzM0s"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hashU43LAyV6"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQzJw1YIlsh_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.metrics import TruePositives, TrueNegatives, FalsePositives, FalseNegatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyoIDh2-3bYe"
      },
      "outputs": [],
      "source": [
        "path_API_autentification_token= '/content/drive/MyDrive/BigData Pneumonia Project'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncE1TajKl516"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = path_API_autentification_token\n",
        "\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "\n",
        "zip_ref = zipfile.ZipFile('chest-xray-pneumonia.zip', 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUWVqEMk2_sZ"
      },
      "outputs": [],
      "source": [
        "train_dir = '/tmp/chest_xray/train'\n",
        "val_dir = '/tmp/chest_xray/val'\n",
        "test_dir = '/tmp/chest_xray/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wew_8n4d3WdC"
      },
      "outputs": [],
      "source": [
        "img_height = 128\n",
        "img_width = 128\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ7ZB7VF3eCQ"
      },
      "outputs": [],
      "source": [
        "train_df = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    color_mode = 'grayscale',\n",
        "    image_size = (img_height,img_width),\n",
        "    batch_size = batch_size\n",
        ")\n",
        "\n",
        "val_df = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    color_mode = 'grayscale',\n",
        "    image_size = (img_height,img_width),\n",
        "    batch_size = batch_size\n",
        ")\n",
        "\n",
        "test_df = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    color_mode = 'grayscale',\n",
        "    image_size = (img_height,img_width),\n",
        "    batch_size = batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ0GP_TC9MMP"
      },
      "outputs": [],
      "source": [
        "train_labels = []\n",
        "test_labels = []\n",
        "val_labels = []\n",
        "\n",
        "for images, labels in train_df.unbatch():\n",
        "  train_labels.append(labels.numpy())\n",
        "\n",
        "for images, labels in test_df.unbatch():\n",
        "  test_labels.append(labels.numpy())\n",
        "\n",
        "for images, labels in val_df.unbatch():\n",
        "  val_labels.append(labels.numpy())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Count values of instances per label in train dataset\\n\")\n",
        "print(pd.DataFrame(np.unique(train_labels, return_counts = True), index=['Label', 'count'], columns=['Normal','Pneumonia']))\n",
        "print(\"\\n\\nCount values of instances per label in test dataset\\n\")\n",
        "print(pd.DataFrame(np.unique(test_labels, return_counts = True), index=['Label', 'count'], columns=['Normal','Pneumonia']))\n",
        "print(\"\\n\\nCount values of instances per label in validation dataset\\n\")\n",
        "print(pd.DataFrame(np.unique(val_labels, return_counts = True), index=['Label', 'count'], columns=['Normal','Pneumonia']))"
      ],
      "metadata": {
        "id": "YZg8ROXXOQGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egzl5NOV4h5D"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_df.take(1):\n",
        "    for i in range(9):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(np.squeeze(images[i].numpy().astype(\"uint8\")))\n",
        "        plt.title(train_df.class_names[labels[i]])\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w_wa9fi5PIF"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_df = train_df.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_df = val_df.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_df = test_df.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_666G3J4lc5w"
      },
      "outputs": [],
      "source": [
        "!pip install https://sourceforge.net/projects/analytics-zoo/files/dllib-py-spark3/bigdl_dllib_spark3-0.14.0b20211107-py3-none-manylinux1_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcnmozzgbobP"
      },
      "outputs": [],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glxXpuN3mUCP"
      },
      "outputs": [],
      "source": [
        "from bigdl.dllib.nn.layer import *\n",
        "from bigdl.dllib.nn.criterion import *\n",
        "from bigdl.dllib.optim.optimizer import *\n",
        "from bigdl.dllib.nncontext import *\n",
        "from bigdl.dllib import keras\n",
        "from bigdl.dllib.keras.layers import *\n",
        "from bigdl.dllib.keras.models import *\n",
        "from bigdl.dllib.nnframes import *\n",
        "from bigdl.dllib.nn.criterion import *\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import load_img\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NNIFRU1buFN"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dr-aMyymFnv"
      },
      "outputs": [],
      "source": [
        "#spark context\n",
        "sc = init_nncontext(cluster_mode=\"local\")\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exbq13kimssd"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "#ImageDataGeneratorthực hiện tăng cường dữ liệu và chuẩn bị các lô hình ảnh để đào tạo và đánh giá\n",
        "#data transformation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, vertical_flip=True)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, vertical_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, vertical_flip=True)\n",
        "\n",
        "#import the data\n",
        "#batch_size xác định số lượng mẫu mỗi lô trong quá trình đào tạo hoặc đánh giá\n",
        "#target_size chỉ định kích thước mà hình ảnh sẽ được thay đổi kích thước\n",
        "\n",
        "train_generator= train_datagen.flow_from_directory(train_dir, target_size = (64,64), batch_size = 64, class_mode=\"binary\" )\n",
        "validation_generator = validation_datagen.flow_from_directory(val_dir, target_size = (64,64), batch_size =  batch_size, class_mode=\"binary\")\n",
        "test_generator = test_datagen.flow_from_directory(test_dir, target_size = (64,64), batch_size= batch_size, class_mode=\"binary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUj8d9ussDkB"
      },
      "outputs": [],
      "source": [
        "X_train, Y_train = next(train_generator)\n",
        "X_val, Y_val = next(validation_generator)\n",
        "X_test, Y_test = next(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QHLJ1wW2Ezk"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Reshape, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Reshape((64, 64, 3), input_shape=(64, 64, 3)))\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\", name=\"conv1\"))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\", name=\"conv2\"))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\", name=\"conv3\"))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation=\"relu\", name=\"fc1\"))\n",
        "model.add(Dense(2, activation=\"softmax\", name=\"fc2\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0O2F1PU3CTC"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXC_iOaV3GKp"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    X_train, Y_train,\n",
        "    epochs=30,\n",
        "    batch_size=50,\n",
        "    validation_data=(X_val, Y_val)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X63LEKB9zUt"
      },
      "outputs": [],
      "source": [
        "accuracy = model.evaluate(X_test, Y_test, batch_size=20)\n",
        "print(\"Loss: \", accuracy[0])\n",
        "print(\"Accuracy: \", accuracy[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('bigdl.h5')"
      ],
      "metadata": {
        "id": "0Gv51XFePz1C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}